{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project-2-DL_Web_scraping&Classify_jeans&trouser.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Spy_32zL2-t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "162a3570-37e3-46e6-e87b-b0e9b330aea6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba1nt8Bh0qrQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f933b78-7129-4430-c4e5-23bf89f9df17"
      },
      "source": [
        "!unzip -q \"/content/drive/MyDrive/project/Dataset/jeans&trousersimagedatagenerator.zip\""
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replace base/testing_data/jeans/jeans (1).jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8ARKzuxMJct"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obl2IoP4hTA8"
      },
      "source": [
        "**Set Path for Train,Test,Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-MHvelGMRfn"
      },
      "source": [
        "path = '/content/base'\n",
        "train_dir = '/content/base/training_data'\n",
        "validation_dir = '/content/base/validation_data'\n",
        "test_dir = '/content/base/testing_data'"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9N4diUahMUu"
      },
      "source": [
        "train_jeans_dir = '/content/base/training_data/jeans'\r\n",
        "train_trousers_dir = '/content/base/training_data/trousers'\r\n",
        "\r\n",
        "validation_jeans_dir = '/content/base/validation_data/jeans'\r\n",
        "validation_trousers_dir ='/content/base/validation_data/trousers'\r\n",
        "\r\n",
        "test_jeans_dir = '/content/base/testing_data/jeans'\r\n",
        "test_trousers_dir = '/content/base/testing_data/trousers'"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z00nxw_Osner"
      },
      "source": [
        "if K.image_data_format() == 'channels_first':\r\n",
        "    input_shape = (3, img_width, img_height)\r\n",
        "else:\r\n",
        "    input_shape = (img_width, img_height, 3)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IOvdrbDh_04"
      },
      "source": [
        "num_jeans_train = len(os.listdir(train_jeans_dir))\r\n",
        "num_trousers_train = len(os.listdir(train_trousers_dir))\r\n",
        "\r\n",
        "num_jeans_validation = len(os.listdir(validation_jeans_dir))\r\n",
        "num_trousers_validation = len(os.listdir(validation_trousers_dir))\r\n",
        "\r\n",
        "num_jeans_test = len(os.listdir(test_jeans_dir))\r\n",
        "num_trousers_test = len(os.listdir(test_trousers_dir))"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJufmdbxjdTI"
      },
      "source": [
        "**Printing Total Numbers of Testing, Training and validation images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJKm9cWWieK6",
        "outputId": "33ce4703-cc0a-44c9-a426-b715bf91bc9f"
      },
      "source": [
        "print('Total Training Images of Jeanss',num_jeans_train)\r\n",
        "print('Total Training Images of Trouserss',num_trousers_train)\r\n",
        "print('\\n************************\\n')\r\n",
        "print('Total Validation Images of Jeanss',num_jeans_validation)\r\n",
        "print('Total Validation Imagges of Trouserss',num_trousers_validation)\r\n",
        "print('\\n************************\\n')\r\n",
        "print('Total Testing Images of Jeanss',num_jeans_test)\r\n",
        "print('Total Testing Images of Trouserss',num_trousers_test)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Training Images of Jeanss 119\n",
            "Total Training Images of Trouserss 120\n",
            "\n",
            "************************\n",
            "\n",
            "Total Validation Images of Jeanss 119\n",
            "Total Validation Imagges of Trouserss 120\n",
            "\n",
            "************************\n",
            "\n",
            "Total Testing Images of Jeanss 119\n",
            "Total Testing Images of Trouserss 120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rmqpnI5ix77"
      },
      "source": [
        "total_train = num_jeans_train+num_trousers_train\r\n",
        "total_validation = num_jeans_validation+num_trousers_validation\r\n",
        "total_test = num_jeans_test+num_trousers_test"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-VWRLCujGMy",
        "outputId": "9bd8ee22-7a02-49bc-dee2-1cc94ff4d5b5"
      },
      "source": [
        "print('Total Training Images',total_train)\r\n",
        "print('Total Validation Images',total_validation)\r\n",
        "print('Total Testing Images',total_test)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Training Images 239\n",
            "Total Validation Images 239\n",
            "Total Testing Images 239\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL7ekBx9jlNS"
      },
      "source": [
        "**Defining Batch-size and image_shape for deep learning model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyA0C6NRjK2C"
      },
      "source": [
        "BATCH_SIZE = 120\r\n",
        "IMG_SHAPE  = 150"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53D46RvHjqpv"
      },
      "source": [
        "**Function for plotting images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKzhiHlujQeV"
      },
      "source": [
        "def plotImages(images_arr):\r\n",
        "    fig, axes = plt.subplots(1, 5, figsize=(20,20))\r\n",
        "    axes = axes.flatten()\r\n",
        "    for img, ax in zip(images_arr, axes):\r\n",
        "        ax.imshow(img)\r\n",
        "    plt.tight_layout()\r\n",
        "    plt.show()"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEtSZMy6j1xb"
      },
      "source": [
        "**Applying Data Agumentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqF74qpVjzRY",
        "outputId": "55fe2788-0b54-455a-cc0d-c4c637fc9d32"
      },
      "source": [
        "image_gen_train = ImageDataGenerator(rescale=1./255,\r\n",
        "                                     rotation_range=40,\r\n",
        "                                     width_shift_range=0.2,\r\n",
        "                                     height_shift_range=0.2,\r\n",
        "                                     shear_range=0.2,\r\n",
        "                                     zoom_range=0.2,\r\n",
        "                                     horizontal_flip=True,\r\n",
        "                                     fill_mode='nearest')\r\n",
        "\r\n",
        "train_data_gen = image_gen_train.flow_from_directory(train_dir,\r\n",
        "                                                     batch_size=batch_size,\r\n",
        "                                                     shuffle=True,\r\n",
        "                                                     target_size=(img_width, img_height),\r\n",
        "                                                     class_mode='binary')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 239 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "Pfyv0ND2kPrT",
        "outputId": "a03b8ccf-a7e3-4bef-c92b-3624f684c7de"
      },
      "source": [
        "augmented_images = [train_data_gen[0][0][0] for i in range(5)]\r\n",
        "plotImages(augmented_images)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnidentifiedImageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-9fd91db604af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maugmented_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_data_gen\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplotImages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmented_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-9fd91db604af>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maugmented_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_data_gen\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplotImages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmented_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     63\u001b[0m         index_array = self.index_array[self.batch_size * idx:\n\u001b[1;32m     64\u001b[0m                                        self.batch_size * (idx + 1)]\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m    228\u001b[0m                            \u001b[0mcolor_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                            \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                            interpolation=self.interpolation)\n\u001b[0m\u001b[1;32m    231\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;31m# Pillow images should be closed after `load_img`,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    112\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;31m# if image is not already an 8-bit, 16-bit or 32-bit grayscale image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2860\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2861\u001b[0m     raise UnidentifiedImageError(\n\u001b[0;32m-> 2862\u001b[0;31m         \u001b[0;34m\"cannot identify image file %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2863\u001b[0m     )\n\u001b[1;32m   2864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BytesIO object at 0x7f22d62d6678>"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQQY6W02khbt"
      },
      "source": [
        "**Pre-Processing for testing and validation data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHZDUx1-lQCs"
      },
      "source": [
        "image_gen_val = ImageDataGenerator(rescale=1./255)\r\n",
        "\r\n",
        "val_data_gen = image_gen_val.flow_from_directory(batch_size=BATCH_SIZE,\r\n",
        "                                                 directory=validation_dir,\r\n",
        "                                                 target_size=(IMG_SHAPE, IMG_SHAPE),\r\n",
        "                                                 class_mode='binary')\r\n",
        "\r\n",
        "image_gen_test = ImageDataGenerator(rescale=1./255)\r\n",
        "\r\n",
        "test_data_gen = image_gen_test.flow_from_directory(batch_size=BATCH_SIZE,\r\n",
        "                                                 directory=test_dir,\r\n",
        "                                                 target_size=(IMG_SHAPE, IMG_SHAPE),\r\n",
        "                                                 class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeCdlfgzoQu4"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dropout,Conv2D,Flatten,Dense,Activation,MaxPooling2D\r\n",
        "from keras import backend as k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQVWwcZrqtC9"
      },
      "source": [
        "from tensorflow.compat.v1 import ConfigProto\r\n",
        "from tensorflow.compat.v1 import InteractiveSession\r\n",
        "\r\n",
        "config = ConfigProto()\r\n",
        "config.gpu_options.allow_growth = True\r\n",
        "session = InteractiveSession(config=config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpxv3XhmoSwt"
      },
      "source": [
        "model=Sequential()\r\n",
        "model.add(Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=(150, 150, 3)))\r\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(Conv2D(32,kernel_size=(3,3),activation='relu'))\r\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(Conv2D(64,(3,3),activation='relu'))\r\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(64,activation='relu'))\r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(Dense(1))\r\n",
        "model.add(Activation('sigmoid'))\r\n",
        "\r\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\r\n",
        "print(model.summary())    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "za1FckBkopNi"
      },
      "source": [
        "import keras\r\n",
        "import tensorflow as tf\r\n",
        "tf.keras.utils.plot_model(model,to_file=\"my_model.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knYzWIP4rv4J"
      },
      "source": [
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc00Ppael5Uu"
      },
      "source": [
        "epochs=100\r\n",
        "history = model.fit(\r\n",
        "    train_data_gen,\r\n",
        "    steps_per_epoch=int(np.ceil(total_train / float(BATCH_SIZE))),\r\n",
        "    epochs=epochs,\r\n",
        "    validation_data=val_data_gen,\r\n",
        "    validation_steps=int(np.ceil(total_validation / float(BATCH_SIZE))),\r\n",
        "    verbose= 1\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rxuPOfhPFZG"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "history_dict=history.history\r\n",
        "loss_values=history_dict['loss']\r\n",
        "val_loss_values=history_dict['val_loss']\r\n",
        "epochs=range(1,len(loss_values)+1)\r\n",
        "line1=plt.plot(epochs,val_loss_values,label='validation/test loss')\r\n",
        "line2=plt.plot(epochs,loss_values,label='Training loss')\r\n",
        "plt.setp(line1,linewidth=2.0,marker='+',markersize=10.0)\r\n",
        "plt.setp(line2,linewidth=2.0,marker='4',markersize=10.0)\r\n",
        "plt.xlabel('Epochs')\r\n",
        "plt.ylabel('Loss')\r\n",
        "plt.grid(True)\r\n",
        "plt.legend()\r\n",
        "plt.show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsAvFgRgONiW"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "history_dict=history.history\r\n",
        "acc_values=history_dict['accuracy']\r\n",
        "val_acc_values=history_dict['val_accuracy']\r\n",
        "epochs=range(1,len(loss_values)+1)\r\n",
        "line1=plt.plot(epochs,val_acc_values,label='validation/Test Accuracy ')\r\n",
        "line2=plt.plot(epochs,acc_values,label='Training Accuracy')\r\n",
        "plt.setp(line1,linewidth=2.0,marker='+',markersize=10.0)\r\n",
        "plt.setp(line2,linewidth=2.0,marker='4',markersize=10.0)\r\n",
        "plt.xlabel('Epochs')\r\n",
        "plt.ylabel('Accuracy')\r\n",
        "plt.grid(True)\r\n",
        "plt.legend()\r\n",
        "plt.show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h-U5ZNCtt7B"
      },
      "source": [
        "history.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ6j1Xhgtt_d"
      },
      "source": [
        "model.save('Jeans&Trousers.h5')\r\n",
        "print('model save')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBM3HXTFtuEf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zJG4US_tqHg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ph9DbAshtqL0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}